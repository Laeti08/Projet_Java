{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOupn9TmEKKp6M3gPXn+WVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laeti08/Projet_Java/blob/main/Projet_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2tlSiVCypC1"
      },
      "source": [
        "Projet de NLP\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4Ps8HoSy04I"
      },
      "source": [
        "# Importation de fichier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWVKJ2WIvRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f8a4aea-a35a-4bd1-a2d5-0089a7d1a0ca"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFdZxr7ny8pZ"
      },
      "source": [
        "# Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Rsa6ZpzFVO"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWhOzsbMzCvR"
      },
      "source": [
        "# Importations des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHpyfxNDzfBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42551fa2-5296-418b-960e-38ad901780e8"
      },
      "source": [
        "data = pd.read_csv(\"gdrive/MyDrive/Projet/donnee_projet.csv\",encoding = \"ISO-8859-1\", header=None, names=[\"target\", \"ids\", \"date\", \"flag\", \"username\", \"text\"])\r\n",
        "data.head"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of          target  ...                                               text\n",
              "0             0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1             0  ...  is upset that he can't update his Facebook by ...\n",
              "2             0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3             0  ...    my whole body feels itchy and like its on fire \n",
              "4             0  ...  @nationwideclass no, it's not behaving at all....\n",
              "...         ...  ...                                                ...\n",
              "1599995       4  ...  Just woke up. Having no school is the best fee...\n",
              "1599996       4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997       4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998       4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999       4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 6 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbITHloS55bW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENKhGQU558JJ"
      },
      "source": [
        "data=data.drop(columns=[\"ids\", \"date\",\"flag\",\"username\"])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EaL0NJEz6qRa",
        "outputId": "4e22657e-c8df-4fe6-f4bf-704290711f8f"
      },
      "source": [
        "data.head()\r\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target                                               text\n",
              "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1       0  is upset that he can't update his Facebook by ...\n",
              "2       0  @Kenichan I dived many times for the ball. Man...\n",
              "3       0    my whole body feels itchy and like its on fire \n",
              "4       0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJcVHg4J7n1I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBkRQr8J7oFR",
        "outputId": "733f5650-3811-4a94-db8d-456b69847ba6"
      },
      "source": [
        "list_tweet_pos=data_pos['text']\r\n",
        "print(list_tweet_pos[0:10])\r\n",
        "list_tweet_neg=data_neg['text']\r\n",
        "list_tweet_neg[0:10]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800000                                    [love, u, guy, r]\n",
            "800001       [im, meet, one, besti, tonight, ., cant, girl]\n",
            "800002    [darealsunisakim, thank, twitter, add, ., suni...\n",
            "800003    [sick, realli, cheap, hurt, much, eat, real, f...\n",
            "800004                                    [effect, everyon]\n",
            "800005    [productoffear, tell, burst, laugh, realli, lo...\n",
            "800006      [than, respons, ., ihad, alreadi, find, answer]\n",
            "800007    [keepinupwkri, jealous, ., hope, great, time, ...\n",
            "800008    [tommcfli, ah, ., congrat, mr, fletcher, final...\n",
            "800009    [respond, stupid, cat, help, type, ., forgiv, ...\n",
            "Name: text, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [switchfoot, http, awww, ., bummer, ., shoulda...\n",
              "1    [upset, updat, facebook, text, might, cri, res...\n",
              "2    [kenichan, dive, mani, time, ball, ., manag, s...\n",
              "3               [whole, bodi, feel, itchi, like, fire]\n",
              "4    [nationwideclass, ., behav, ., mad, ., ., see, .]\n",
              "5                              [kwesidei, whole, crew]\n",
              "6                                          [need, hug]\n",
              "7    [loltrish, hey, long, time, see, ., rain, bit,...\n",
              "8                                               [nope]\n",
              "9                            [twittera, que, muera, .]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWbtyNohP7QR"
      },
      "source": [
        "Elimination de la ponctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "bN3ginxlN7qr",
        "outputId": "5cce1b17-99d0-4ac0-805e-222fdbb2178f"
      },
      "source": [
        "for i in range(10):\r\n",
        "  list_tweet_neg[i]=list_tweet_neg[i].replace(\"'\",\".\").replace(\"?\",\".\").replace(\"!\",\".\").replace(\",\",\".\").replace(\";\",\".\").replace(\"/\",\".\")\r\n",
        "list_tweet_neg[0:10]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-4b0080b31eb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mlist_tweet_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_tweet_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlist_tweet_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfqxGpIQQARS"
      },
      "source": [
        "Tokenisation et minuscule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1N1URw-QAit",
        "outputId": "89bca260-cffe-429f-ee8c-db0f0ac43d4a"
      },
      "source": [
        "for i in range(10):\r\n",
        "  list_tweet_neg[i] = nltk.word_tokenize(list_tweet_neg[i])  # tokenize string to words\r\n",
        "  list_tweet_neg[i] = [ ch.lower() for ch in list_tweet_neg[i]\r\n",
        "             if ch.isalpha()\r\n",
        "             or ch == '.']\r\n",
        "list_tweet_neg[0:10]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [switchfoot, http, awww, ., a, bummer, ., you,...\n",
              "1    [is, upset, that, he, update, his, facebook, b...\n",
              "2    [kenichan, i, dived, many, times, for, the, ba...\n",
              "3    [my, whole, body, feels, itchy, and, like, its...\n",
              "4    [nationwideclass, no, ., not, behaving, at, al...\n",
              "5                    [kwesidei, not, the, whole, crew]\n",
              "6                                       [need, a, hug]\n",
              "7    [loltrish, hey, long, time, no, see, ., rains,...\n",
              "8                               [nope, they, have, it]\n",
              "9                        [twittera, que, me, muera, .]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN1aDpT_RhUw"
      },
      "source": [
        "Stopword"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ9Q3cPjP_Xo",
        "outputId": "913e7990-5caa-459a-e1b6-e8493dae66fc"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "for i in range(10):\r\n",
        "  token=list_tweet_neg[i]\r\n",
        "  #print(token)\r\n",
        "  token=[w for w in token if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "  list_tweet_neg[i]=token\r\n",
        "list_tweet_neg[0:10]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [switchfoot, http, awww, ., bummer, ., shoulda...\n",
              "1    [upset, update, facebook, texting, might, cry,...\n",
              "2    [kenichan, dived, many, times, ball, ., manage...\n",
              "3              [whole, body, feels, itchy, like, fire]\n",
              "4    [nationwideclass, ., behaving, ., mad, ., ., s...\n",
              "5                              [kwesidei, whole, crew]\n",
              "6                                          [need, hug]\n",
              "7    [loltrish, hey, long, time, see, ., rains, bit...\n",
              "8                                               [nope]\n",
              "9                            [twittera, que, muera, .]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyJaveduVkhl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwJ-KxAtVkyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b84b9a-2a8e-4a8e-e02c-824b464f1f11"
      },
      "source": [
        "from nltk.stem.snowball import EnglishStemmer\r\n",
        "\r\n",
        "stemmer = EnglishStemmer()\r\n",
        "for i in range(10):\r\n",
        "  tokens=list_tweet_neg[i]\r\n",
        "  tokens_stem = [stemmer.stem(w) for w in tokens]\r\n",
        "  #print(tokens_stem)\r\n",
        "  list_tweet_neg[i]=tokens_stem\r\n",
        "list_tweet_neg[0:10]\r\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [switchfoot, http, awww, ., bummer, ., shoulda...\n",
              "1    [upset, updat, facebook, text, might, cri, res...\n",
              "2    [kenichan, dive, mani, time, ball, ., manag, s...\n",
              "3               [whole, bodi, feel, itchi, like, fire]\n",
              "4    [nationwideclass, ., behav, ., mad, ., ., see, .]\n",
              "5                              [kwesidei, whole, crew]\n",
              "6                                          [need, hug]\n",
              "7    [loltrish, hey, long, time, see, ., rain, bit,...\n",
              "8                                               [nope]\n",
              "9                            [twittera, que, muera, .]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVjXLtoj1WcC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agYNbvP81WnK",
        "outputId": "c60cec7b-d2b7-40d3-d0bf-0e67afd852eb"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "nltk.download('wordnet')\r\n",
        "lm = nltk.WordNetLemmatizer()\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "  tokens=list_tweet_neg[i]\r\n",
        "  #print(tokens)\r\n",
        "  tokens_lema = [lm.lemmatize(w) for w in tokens]\r\n",
        "  #print(tokens_lema)\r\n",
        "  list_tweet_neg[i]=tokens_lema\r\n",
        "list_tweet_neg[0:10]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [switchfoot, http, awww, ., bummer, ., shoulda...\n",
              "1    [upset, updat, facebook, text, might, cri, res...\n",
              "2    [kenichan, dive, mani, time, ball, ., manag, s...\n",
              "3               [whole, bodi, feel, itchi, like, fire]\n",
              "4    [nationwideclass, ., behav, ., mad, ., ., see, .]\n",
              "5                              [kwesidei, whole, crew]\n",
              "6                                          [need, hug]\n",
              "7    [loltrish, hey, long, time, see, ., rain, bit,...\n",
              "8                                               [nope]\n",
              "9                            [twittera, que, muera, .]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msvfd3Hw-BeR",
        "outputId": "1dced7c0-3a38-4ade-c8b9-b95fa17a8976"
      },
      "source": [
        "for i in range(800000,800010):\r\n",
        "  list_tweet_pos[i]=list_tweet_pos[i].replace(\"'\",\".\").replace(\"?\",\".\").replace(\"!\",\".\").replace(\",\",\".\").replace(\";\",\".\").replace(\"/\",\".\")\r\n",
        "list_tweet_pos[0:10]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000         I LOVE @Health4UandPets u guys r the best.. \n",
              "800001    im meeting up with one of my besties tonight. ...\n",
              "800002    @DaRealSunisaKim Thanks for the Twitter add. S...\n",
              "800003    Being sick can be really cheap when it hurts t...\n",
              "800004      @LovesBrooklyn2 he has that effect on everyone \n",
              "800005    @ProductOfFear You can tell him that I just bu...\n",
              "800006    @r_keith_hill Thans for your response. Ihad al...\n",
              "800007    @KeepinUpWKris I am so jealous. hope you had a...\n",
              "800008    @tommcfly ah. congrats mr fletcher for finally...\n",
              "800009    @e4VoIP I RESPONDED  Stupid cat is helping me ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-qOsF8O-J5Q",
        "outputId": "07e90b4e-1fca-4ac9-d6c1-9389e7209d44"
      },
      "source": [
        "for i in range(800000,800010):\r\n",
        "  list_tweet_pos[i] = nltk.word_tokenize(list_tweet_pos[i])  # tokenize string to words\r\n",
        "  list_tweet_pos[i] = [ ch.lower() for ch in list_tweet_pos[i]\r\n",
        "             if ch.isalpha()\r\n",
        "             or ch == '.']\r\n",
        "list_tweet_pos[0:10]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000                           [i, love, u, guys, r, the]\n",
              "800001    [im, meeting, up, with, one, of, my, besties, ...\n",
              "800002    [darealsunisakim, thanks, for, the, twitter, a...\n",
              "800003    [being, sick, can, be, really, cheap, when, it...\n",
              "800004                [he, has, that, effect, on, everyone]\n",
              "800005    [productoffear, you, can, tell, him, that, i, ...\n",
              "800006    [thans, for, your, response, ., ihad, already,...\n",
              "800007    [keepinupwkris, i, am, so, jealous, ., hope, y...\n",
              "800008    [tommcfly, ah, ., congrats, mr, fletcher, for,...\n",
              "800009    [i, responded, stupid, cat, is, helping, me, t...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-QhXHjq-NPr",
        "outputId": "7d5cb73b-c34e-49c7-8c9b-25620b46494e"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.text import Text\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "for i in range(800000,800010):\r\n",
        "  token=list_tweet_pos[i]\r\n",
        "  #print(token)\r\n",
        "  token=[w for w in token if not w in list(nltk.corpus.stopwords.words('english'))]\r\n",
        "  list_tweet_pos[i]=token\r\n",
        "list_tweet_pos[0:10]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000                                   [love, u, guys, r]\n",
              "800001    [im, meeting, one, besties, tonight, ., cant, ...\n",
              "800002    [darealsunisakim, thanks, twitter, add, ., sun...\n",
              "800003    [sick, really, cheap, hurts, much, eat, real, ...\n",
              "800004                                   [effect, everyone]\n",
              "800005    [productoffear, tell, burst, laughing, really,...\n",
              "800006    [thans, response, ., ihad, already, find, answer]\n",
              "800007    [keepinupwkris, jealous, ., hope, great, time,...\n",
              "800008    [tommcfly, ah, ., congrats, mr, fletcher, fina...\n",
              "800009    [responded, stupid, cat, helping, type, ., for...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNetURqP-N6F",
        "outputId": "7c4692cc-1c01-433c-9667-b680984682f8"
      },
      "source": [
        "from nltk.stem.snowball import EnglishStemmer\r\n",
        "\r\n",
        "stemmer = EnglishStemmer()\r\n",
        "for i in range(800000,800010):\r\n",
        "  tokens=list_tweet_pos[i]\r\n",
        "  tokens_stem = [stemmer.stem(w) for w in tokens]\r\n",
        "  #print(tokens_stem)\r\n",
        "  list_tweet_pos[i]=tokens_stem\r\n",
        "list_tweet_pos[0:10]\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000                                    [love, u, guy, r]\n",
              "800001       [im, meet, one, besti, tonight, ., cant, girl]\n",
              "800002    [darealsunisakim, thank, twitter, add, ., suni...\n",
              "800003    [sick, realli, cheap, hurt, much, eat, real, f...\n",
              "800004                                    [effect, everyon]\n",
              "800005    [productoffear, tell, burst, laugh, realli, lo...\n",
              "800006      [than, respons, ., ihad, alreadi, find, answer]\n",
              "800007    [keepinupwkri, jealous, ., hope, great, time, ...\n",
              "800008    [tommcfli, ah, ., congrat, mr, fletcher, final...\n",
              "800009    [respond, stupid, cat, help, type, ., forgiv, ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj0ox7Ux-OL6",
        "outputId": "f110aa7f-1e6f-443b-9ee3-e741fe0ae797"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "nltk.download('wordnet')\r\n",
        "lm = nltk.WordNetLemmatizer()\r\n",
        "\r\n",
        "for i in range(800000,800010):\r\n",
        "  tokens=list_tweet_pos[i]\r\n",
        "  #print(tokens)\r\n",
        "  tokens_lema = [lm.lemmatize(w) for w in tokens]\r\n",
        "  #print(tokens_lema)\r\n",
        "  list_tweet_pos[i]=tokens_lema\r\n",
        "list_tweet_pos[0:10]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000                                    [love, u, guy, r]\n",
              "800001       [im, meet, one, besti, tonight, ., cant, girl]\n",
              "800002    [darealsunisakim, thank, twitter, add, ., suni...\n",
              "800003    [sick, realli, cheap, hurt, much, eat, real, f...\n",
              "800004                                    [effect, everyon]\n",
              "800005    [productoffear, tell, burst, laugh, realli, lo...\n",
              "800006      [than, respons, ., ihad, alreadi, find, answer]\n",
              "800007    [keepinupwkri, jealous, ., hope, great, time, ...\n",
              "800008    [tommcfli, ah, ., congrat, mr, fletcher, final...\n",
              "800009    [respond, stupid, cat, help, type, ., forgiv, ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xiPEuWgEO8b",
        "outputId": "74952148-ad4a-42da-d6a4-62ef4bfac70f"
      },
      "source": [
        "list_neg_unique=set()\r\n",
        "for i in range(10):\r\n",
        "  for j in range(len(list_tweet_neg[i])):\r\n",
        "    list_neg_unique.add(list_tweet_neg[i][j])\r\n",
        "\r\n",
        "print(\"tab_neg_unique:\")\r\n",
        "print(len(list_neg_unique),list_neg_unique)\r\n",
        "\r\n",
        "tab_neg_vecteur=[]\r\n",
        "Vecteur_neg=[]\r\n",
        "\r\n",
        "\r\n",
        "for j in range(10):\r\n",
        "  for word in list_neg_unique:\r\n",
        "    if word in list_tweet_neg[j]:\r\n",
        "      Vecteur_neg.append(list_tweet_neg[j].count(word))\r\n",
        "    else:\r\n",
        "      Vecteur_neg.append(0)\r\n",
        "      \r\n",
        "      \r\n",
        "\r\n",
        "  tab_neg_vecteur.append(Vecteur_neg)\r\n",
        "  Vecteur_neg=[]\r\n",
        "\r\n",
        "print(\"tab_neg_vecteur: \", tab_neg_vecteur[:5])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tab_neg_unique:\n",
            "58 {'time', 'feel', 'que', 'go', 'mani', 'nationwideclass', '.', 'rest', 'fine', 'school', 'bodi', 'bound', 'fire', 'hey', 'save', 'hug', 'today', 'twittera', 'got', 'might', 'lol', 'shoulda', 'manag', 'kwesidei', 'kenichan', 'updat', 'switchfoot', 'bummer', 'facebook', 'itchi', 'like', 'text', 'mad', 'see', 'cri', 'third', 'carr', 'also', 'long', 'david', 'upset', 'blah', 'dive', 'ball', 'loltrish', 'rain', 'http', 'need', 'thank', 'bit', 'crew', 'awww', 'result', 'nope', 'muera', 'behav', 'whole', 'day'}\n",
            "tab_neg_vecteur:  [[0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xyc--pADz5D",
        "outputId": "fce5a494-a95a-4391-a665-4fe7a3d7912e"
      },
      "source": [
        "list_pos_unique=set()\r\n",
        "for i in range(800000,800010):\r\n",
        "  for j in range(len(list_tweet_pos[i])):\r\n",
        "    list_pos_unique.add(list_tweet_pos[i][j])\r\n",
        "\r\n",
        "print(\"tab_pos_unique:\")\r\n",
        "print(len(list_pos_unique),list_pos_unique)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "tab_pos_vecteur=[]\r\n",
        "Vecteur_pos=[]\r\n",
        "\r\n",
        "\r\n",
        "for j in range(800000,800010):\r\n",
        "  for word in list_pos_unique:\r\n",
        "    if word in list_tweet_pos[j]:\r\n",
        "      Vecteur_pos.append(list_tweet_pos[j].count(word))\r\n",
        "    else:\r\n",
        "      Vecteur_pos.append(0)\r\n",
        "      \r\n",
        "      \r\n",
        "\r\n",
        "  tab_pos_vecteur.append(Vecteur_pos)\r\n",
        "  Vecteur_pos=[]\r\n",
        "\r\n",
        "print(\"tab_pos_vecteur: \", tab_pos_vecteur[:5])\r\n",
        "\r\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tab_pos_unique:\n",
            "71 {'laugh', 'time', 'add', 'besti', 'loud', 'join', 'sick', 'great', 'tommcfli', '.', 'tell', 'burst', 'sunisa', 'fletcher', 'hin', 'area', 'realli', 'find', 'meet', 'darealsunisakim', 'make', 'alreadi', 'eat', 'type', 'mr', 'respond', 'effect', 'hope', 'got', 'u', 'tonight', 'friend', 'show', 'r', 'forgiv', 'twitter', 'love', 'everyon', 'hurt', 'like', 'soup', 'than', 'girl', 'dc', 'stupid', 'jealous', 'productoffear', 'guy', 'cant', 'come', 'cheap', 'cat', 'sulk', 'vega', 'food', 'respons', 'plus', 'one', 'thank', 'keepinupwkri', 'help', 'im', 'final', 'much', 'answer', 'error', 'ah', 'ihad', 'real', 'congrat', 'sweetheart'}\n",
            "tab_pos_vecteur:  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vls8uNer_UHc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "d7Ki9UP6_Uwo",
        "outputId": "bb8e394e-82e8-4c08-c46e-e304fc8de155"
      },
      "source": [
        "data_pos['text']=list_tweet_pos\r\n",
        "print(data_pos)\r\n",
        "data_neg['text']=list_tweet_neg\r\n",
        "data_neg"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         target                                               text\n",
            "800000        1                                  [love, u, guy, r]\n",
            "800001        1     [im, meet, one, besti, tonight, ., cant, girl]\n",
            "800002        1  [darealsunisakim, thank, twitter, add, ., suni...\n",
            "800003        1  [sick, realli, cheap, hurt, much, eat, real, f...\n",
            "800004        1                                  [effect, everyon]\n",
            "...         ...                                                ...\n",
            "1599995       1  Just woke up. Having no school is the best fee...\n",
            "1599996       1  TheWDB.com - Very cool to hear old Walt interv...\n",
            "1599997       1  Are you ready for your MoJo Makeover? Ask me f...\n",
            "1599998       1  Happy 38th Birthday to my boo of alll time!!! ...\n",
            "1599999       1  happy #charitytuesday @theNSPCC @SparksCharity...\n",
            "\n",
            "[800000 rows x 2 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[switchfoot, http, awww, ., bummer, ., shoulda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[upset, updat, facebook, text, might, cri, res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[kenichan, dive, mani, time, ball, ., manag, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[nationwideclass, ., behav, ., mad, ., ., see, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799995</th>\n",
              "      <td>0</td>\n",
              "      <td>Sick  Spending my day laying in bed listening ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799996</th>\n",
              "      <td>0</td>\n",
              "      <td>Gmail is down?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799997</th>\n",
              "      <td>0</td>\n",
              "      <td>rest in peace Farrah! So sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799998</th>\n",
              "      <td>0</td>\n",
              "      <td>@Eric_Urbane Sounds like a rival is flagging y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799999</th>\n",
              "      <td>0</td>\n",
              "      <td>has to resit exams over summer...  wishes he w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        target                                               text\n",
              "0            0  [switchfoot, http, awww, ., bummer, ., shoulda...\n",
              "1            0  [upset, updat, facebook, text, might, cri, res...\n",
              "2            0  [kenichan, dive, mani, time, ball, ., manag, s...\n",
              "3            0             [whole, bodi, feel, itchi, like, fire]\n",
              "4            0  [nationwideclass, ., behav, ., mad, ., ., see, .]\n",
              "...        ...                                                ...\n",
              "799995       0  Sick  Spending my day laying in bed listening ...\n",
              "799996       0                                    Gmail is down? \n",
              "799997       0                      rest in peace Farrah! So sad \n",
              "799998       0  @Eric_Urbane Sounds like a rival is flagging y...\n",
              "799999       0  has to resit exams over summer...  wishes he w...\n",
              "\n",
              "[800000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKMVtuvT_xd1"
      },
      "source": [
        "Création du model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "l_Tq2u3h_s6l",
        "outputId": "7e3c8372-f389-4e39-8132-a4c61b2d9a39"
      },
      "source": [
        "data = pd.concat([data_neg, data_pos])\r\n",
        "data"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[switchfoot, http, awww, ., bummer, ., shoulda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[upset, updat, facebook, text, might, cri, res...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[kenichan, dive, mani, time, ball, ., manag, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[nationwideclass, ., behav, ., mad, ., ., see, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>1</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>1</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>1</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>1</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         target                                               text\n",
              "0             0  [switchfoot, http, awww, ., bummer, ., shoulda...\n",
              "1             0  [upset, updat, facebook, text, might, cri, res...\n",
              "2             0  [kenichan, dive, mani, time, ball, ., manag, s...\n",
              "3             0             [whole, bodi, feel, itchi, like, fire]\n",
              "4             0  [nationwideclass, ., behav, ., mad, ., ., see, .]\n",
              "...         ...                                                ...\n",
              "1599995       1  Just woke up. Having no school is the best fee...\n",
              "1599996       1  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997       1  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998       1  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999       1  happy #charitytuesday @theNSPCC @SparksCharity...\n",
              "\n",
              "[1600000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxoLfd4SAS12"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT8ABs27CUn0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHNcQqEQCXw2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}